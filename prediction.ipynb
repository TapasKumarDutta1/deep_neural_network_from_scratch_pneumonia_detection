{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prediction.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TapasKumarDutta1/deep_neural_network_from_scratch_pneumonia_detection/blob/main/prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import gc\n",
        "from numba import jit\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "\n",
        "np.random.seed(777)\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "qR2hIp7jE7ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdQr6c90xtyv"
      },
      "source": [
        "\n",
        "def pad1(a):\n",
        "    \"\"\"Pad the input array 'a' with ones to create a border.\n",
        "\n",
        "    Args:\n",
        "        a (numpy.ndarray): Input array to be padded.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Padded array with a border of ones.\n",
        "    \"\"\"\n",
        "    b = np.ones((a.shape[0] + 2, a.shape[1] + 2))\n",
        "    b[1 : a.shape[0] + 1, 1 : a.shape[1] + 1] = a\n",
        "    b[1:4, [0, -1]] = b[1:4, [1, -2]]\n",
        "    b[[0, -1], :] = b[[1, -2], :]\n",
        "    return b\n",
        "\n",
        "\n",
        "def conv(inp, ker):\n",
        "    \"\"\"Perform convolution operation between input 'inp' and kernel 'ker'.\n",
        "\n",
        "    Args:\n",
        "        inp (numpy.ndarray): Input array for convolution.\n",
        "        ker (numpy.ndarray): Convolution kernel.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Resulting array after convolution.\n",
        "    \"\"\"\n",
        "    inp = pad1(inp)\n",
        "    v_k, h_k = ker.shape\n",
        "    v_i, h_i = inp.shape\n",
        "    out = np.zeros((1 + v_i - v_k, 1 + h_i - h_k))\n",
        "    for i in range(v_i):\n",
        "        for j in range(h_i):\n",
        "            if (i + v_k <= v_i) and (j + h_k <= h_i):\n",
        "                out[i, j] = np.sum(inp[i : i + v_k, j : j + h_k] * ker)\n",
        "    return out\n",
        "\n",
        "\n",
        "def cnt(ground, count):\n",
        "    \"\"\"Count the total occurrences of each unique value in the 'ground' array.\n",
        "\n",
        "    Args:\n",
        "        ground (numpy.ndarray): Input array containing ground truth data.\n",
        "        count (Counter): Counter object with counts of unique values.\n",
        "\n",
        "    Returns:\n",
        "        int: Total number of occurrences of unique values in 'ground'.\n",
        "    \"\"\"\n",
        "    tot = 0\n",
        "    for i in np.unique(ground):\n",
        "        tot += count[i]\n",
        "    return tot\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Compute the sigmoid activation function.\n",
        "\n",
        "    Args:\n",
        "        x (numpy.ndarray): Input array.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Array with elements transformed using the sigmoid function.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(x * -1))\n",
        "\n",
        "\n",
        "def dot(a, b):\n",
        "    \"\"\"Perform element-wise dot product and sum along specific axes.\n",
        "\n",
        "    Args:\n",
        "        a (numpy.ndarray): First input array.\n",
        "        b (numpy.ndarray): Second input array.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Array resulting from the element-wise dot product and sum.\n",
        "    \"\"\"\n",
        "    return np.sum(a * b, axis=(-1, -2, -3))\n",
        "\n",
        "\n",
        "def glorot(size1, in1, out1):\n",
        "    \"\"\"Initialize weights using Glorot (Xavier) initialization.\n",
        "\n",
        "    Args:\n",
        "        size1 (tuple): Size of the weight matrix (kernel).\n",
        "        in1 (int): Number of input units to the layer.\n",
        "        out1 (int): Number of output units from the layer.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Weight matrix initialized using Glorot initialization.\n",
        "    \"\"\"\n",
        "    dst = np.random.normal(0, 1, size1)\n",
        "    mx = np.max(dst)\n",
        "    mn = np.min(dst)\n",
        "\n",
        "    # Scale the values within 0 to 1\n",
        "    dst = (dst - mn) / (mx - mn)\n",
        "\n",
        "    # Scale the values within -1 to 1\n",
        "    dst = (dst - 0.5) * 2\n",
        "\n",
        "    scl = 2.449489742783178 / (np.sqrt(in1 + out1))\n",
        "\n",
        "    # Scale the values within +-scl\n",
        "    dst *= scl\n",
        "    return dst\n",
        "\n",
        "\n",
        "def maxpool(image, kernel):\n",
        "    \"\"\"Perform max pooling operation on the input image.\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): Input image.\n",
        "        kernel (int): Size of the pooling kernel.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the max-pooled image, kernel size, indices of max values,\n",
        "               and the original image.\n",
        "    \"\"\"\n",
        "    bs, l, w, c = image.shape\n",
        "    out = np.zeros((bs, int(math.ceil(l / kernel)), int(math.ceil(w / kernel)), c))\n",
        "    idxs = []\n",
        "    for i in range(int(math.ceil(l / kernel))):\n",
        "        for j in range(int(math.ceil(w / kernel))):\n",
        "            look = image[\n",
        "                :, i * kernel : (i + 1) * kernel, j * kernel : (j + 1) * kernel, :\n",
        "            ]\n",
        "            out[:, i, j, :] = np.max(look, axis=(1, 2))\n",
        "            idxs.append(np.where(np.in1d(look, out[:, i, j, :]))[0])\n",
        "    return out, kernel, np.asarray(idxs), image\n",
        "\n",
        "\n",
        "def convolution(image, kernel, out):\n",
        "    \"\"\"Perform convolution operation on the input image.\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): Input image.\n",
        "        kernel (int): Size of the convolution kernel.\n",
        "        out (int): Number of output feature maps.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the weight matrix (kernel), input image, and the resulting\n",
        "               convolution output.\n",
        "    \"\"\"\n",
        "    bs, s1, s2, s3 = image.shape\n",
        "    b = np.zeros((bs, (s1 - kernel) + 1, (s2 - kernel) + 1, out))\n",
        "    in1 = len(image.ravel()) / bs\n",
        "    out1 = len(b.ravel()) / bs\n",
        "    a = glorot((kernel, kernel, s3, out), in1, out1)\n",
        "    for i in range(out):\n",
        "        for j in range(b.shape[0]):\n",
        "            for k in range(b.shape[1]):\n",
        "                out = dot(a[:, :, :, i], image[:, j : j + kernel, k : k + kernel])\n",
        "                b[:, j, k, i] = out\n",
        "    return a, image, b\n",
        "def flatten(image):\n",
        "    \"\"\"Flatten the input image.\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): Input image of shape (batch_size, height, width, channels).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the original image and the flattened version of the image.\n",
        "    \"\"\"\n",
        "    bs, _, _, _ = image.shape\n",
        "    return image, image.reshape(bs, -1)\n",
        "\n",
        "\n",
        "def dense(input1, out):\n",
        "    \"\"\"Perform dense (fully-connected) layer operation.\n",
        "\n",
        "    Args:\n",
        "        input1 (numpy.ndarray): Input array of shape (batch_size, num_input_units).\n",
        "        out (int): Number of output units in the dense layer.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the weight matrix, input array, and the output after\n",
        "               applying the dense layer with sigmoid activation.\n",
        "    \"\"\"\n",
        "    bs, b = input1.shape\n",
        "    in1 = b\n",
        "    out1 = out\n",
        "    weights = glorot((b, out), in1, out1)\n",
        "    out = np.dot(input1, weights)\n",
        "    return weights, input1, sigmoid(out)\n",
        "\n",
        "\n",
        "def loss(true, pred):\n",
        "    \"\"\"Calculate the binary cross-entropy loss.\n",
        "\n",
        "    Args:\n",
        "        true (numpy.ndarray): True labels (ground truth) of shape (batch_size,).\n",
        "        pred (numpy.ndarray): Predicted probabilities of shape (batch_size,).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Array containing the computed loss for each sample in the batch.\n",
        "    \"\"\"\n",
        "    return -1 * ((true * np.log(pred)) + ((1 - true) * (np.log(1 - pred))))\n",
        "\n",
        "\n",
        "def loss_back(true, pred):\n",
        "    \"\"\"Compute the gradient of the loss with respect to the predicted probabilities.\n",
        "\n",
        "    Args:\n",
        "        true (numpy.ndarray): True labels (ground truth) of shape (batch_size,).\n",
        "        pred (numpy.ndarray): Predicted probabilities of shape (batch_size,).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Gradient of the loss with respect to the predicted probabilities.\n",
        "    \"\"\"\n",
        "    return pred - true\n",
        "\n",
        "\n",
        "def rotate_180(array):\n",
        "    \"\"\"Rotate the 2D array by 180 degrees.\n",
        "\n",
        "    Args:\n",
        "        array (numpy.ndarray): Input 2D array.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Rotated array by 180 degrees.\n",
        "    \"\"\"\n",
        "    M, N = array.shape\n",
        "    out = np.zeros_like(array)\n",
        "    for i in range(M):\n",
        "        for j in range(N):\n",
        "            out[i, N - 1 - j] = array[M - 1 - i, j]\n",
        "    return out\n",
        "\n",
        "\n",
        "def maxpool_derivative(derivative, kernel, idxs):\n",
        "    \"\"\"Compute the derivative of the max-pooling operation.\n",
        "\n",
        "    Args:\n",
        "        derivative (numpy.ndarray): Derivative of the output after max-pooling.\n",
        "        kernel (int): Size of the max-pooling kernel.\n",
        "        idxs (numpy.ndarray): Indices of the max values from the max-pooling operation.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Derivative of the input before max-pooling.\n",
        "    \"\"\"\n",
        "    bs, l, w, c = derivative.shape\n",
        "    out = np.zeros((bs, int(l * kernel), int(w * kernel), c))\n",
        "    en = 0\n",
        "    for i in range(l):\n",
        "        for j in range(w):\n",
        "            look = np.zeros((bs, kernel, kernel, c))\n",
        "            place = idxs[en]\n",
        "            look.ravel()[place] = 1\n",
        "            look *= derivative[:, i, j, :].reshape(bs, 1, 1, c)\n",
        "            en += 1\n",
        "            out[:, i * kernel : (i + 1) * kernel, j * kernel : (j + 1) * kernel, :] = look\n",
        "    return out\n",
        "\n",
        "\n",
        "def cng(grad, wts, x):\n",
        "    \"\"\"Compute the gradient of the convolutional operation with respect to the input.\n",
        "\n",
        "    Args:\n",
        "        grad (numpy.ndarray): Gradient of the output of the current layer.\n",
        "        wts (numpy.ndarray): Weights (kernels) of the current layer.\n",
        "        x (numpy.ndarray): Input of the current layer.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Gradient of the input with respect to the convolutional operation.\n",
        "    \"\"\"\n",
        "    bs, s1, s2, s3 = x.shape\n",
        "    grad1 = np.zeros((bs, s1 + 1, s2 + 1, grad.shape[-1]))\n",
        "\n",
        "    grad1[:, 1:s1, 1:s2, :] = grad\n",
        "\n",
        "    out = np.zeros_like(x)\n",
        "    p, q, r, s = wts.shape\n",
        "\n",
        "    for l in range(s):\n",
        "        for k in range(r):\n",
        "            for i in range(s2):\n",
        "                for j in range(s1):\n",
        "                    out[:, i, j, k] += dot(\n",
        "                        rotate_180(wts[:, :, k, l]), grad1[:, i : i + p, j : j + q, l]\n",
        "                    )\n",
        "    return out\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"Compute the derivative of the sigmoid function.\n",
        "\n",
        "    Args:\n",
        "        x (numpy.ndarray): Input array.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Derivative of the sigmoid function with respect to the input.\n",
        "    \"\"\"\n",
        "    out = np.exp(-x) * (sigmoid(x) ** 2)\n",
        "    return out\n",
        "\n",
        "\n",
        "def conv(input, kernel):\n",
        "    \"\"\"Perform 2D convolution operation on the input using the given kernel.\n",
        "\n",
        "    Args:\n",
        "        input (numpy.ndarray): Input array of shape (batch_size, input_height, input_width, input_channels).\n",
        "        kernel (numpy.ndarray): Convolution kernel of shape (kernel_height, kernel_width, input_channels, output_channels).\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Convolved output of shape (output_height, output_width, output_channels).\n",
        "    \"\"\"\n",
        "    bs, s1, s2, z = input.shape\n",
        "    _, q1, q2, x = kernel.shape\n",
        "    b = np.zeros(((s1 - q1) + 1, (s2 - q2) + 1, z, x))\n",
        "    for l in range(z):\n",
        "        for y in range(x):\n",
        "            for j in range(b.shape[0]):\n",
        "                for k in range(b.shape[1]):\n",
        "                    out = (\n",
        "                        dot(kernel[:, :, :, y], input[:, j : j + q1, k : k + q2, l])\n",
        "                        / input.shape[0]\n",
        "                    )\n",
        "                    b[j, k, l, y] = out\n",
        "    return b\n",
        "\n",
        "\n",
        "def calculate_grad(layers, true):\n",
        "    \"\"\"Calculate gradients for each layer in the neural network.\n",
        "\n",
        "    Args:\n",
        "        layers (list): List of dictionaries representing each layer in the network.\n",
        "        true (numpy.ndarray): True labels or target values.\n",
        "\n",
        "    Returns:\n",
        "        list: Updated list of dictionaries with gradients computed for each layer.\n",
        "    \"\"\"\n",
        "    for layer in layers:\n",
        "        try:\n",
        "            wts, inp, out, type, pos = (\n",
        "                layer[\"weight\"],\n",
        "                layer[\"input\"],\n",
        "                layer[\"output\"],\n",
        "                layer[\"type\"],\n",
        "                layer[\"position\"],\n",
        "            )\n",
        "        except:\n",
        "            try:\n",
        "                out, shp, type, pos = (\n",
        "                    layer[\"output\"],\n",
        "                    layer[\"shape\"],\n",
        "                    layer[\"type\"],\n",
        "                    layer[\"position\"],\n",
        "                )\n",
        "            except:\n",
        "                out, type, pos, shp, index = (\n",
        "                    layer[\"output\"],\n",
        "                    layer[\"type\"],\n",
        "                    layer[\"position\"],\n",
        "                    layer[\"kernel\"],\n",
        "                    layer[\"index\"],\n",
        "                )\n",
        "        for i in range(0, pos):\n",
        "            try:\n",
        "                wts1, inp1, out1, type1, pos1 = (\n",
        "                    layers[i][\"weight\"],\n",
        "                    layers[i][\"input\"],\n",
        "                    layers[i][\"output\"],\n",
        "                    layers[i][\"type\"],\n",
        "                    layers[i][\"position\"],\n",
        "                )\n",
        "            except:\n",
        "                try:\n",
        "                    out1, shp1, type1, pos1 = (\n",
        "                        layers[i][\"output\"],\n",
        "                        layers[i][\"shape\"],\n",
        "                        layers[i][\"type\"],\n",
        "                        layers[i][\"position\"],\n",
        "                    )\n",
        "                except:\n",
        "                    inp1, out1, type1, pos1, shp1, index1 = (\n",
        "                        layers[i][\"input\"],\n",
        "                        layers[i][\"output\"],\n",
        "                        layers[i][\"type\"],\n",
        "                        layers[i][\"position\"],\n",
        "                        layers[i][\"kernel\"],\n",
        "                        layers[i][\"index\"],\n",
        "                    )\n",
        "            if i == 0:\n",
        "                grad = loss_back(true, out1)\n",
        "                ot = sigmoid_derivative(out1)\n",
        "                grad *= ot\n",
        "            if type1 == \"out\" and type != \"out\" and i != pos - 1:\n",
        "                grad = np.dot(grad, wts1.T)\n",
        "            if type1 == \"max\" and i != pos - 1:\n",
        "                grad = maxpool_derivative(grad, shp1, index1)\n",
        "            if type1 == \"flatten\" and i != pos - 1:\n",
        "                grad = grad.reshape(shp1)\n",
        "            if type1 == \"conv\" and i != pos - 1:\n",
        "                grad = cng(grad, wts1, inp1)\n",
        "        if type == \"out\" or type == \"dense\":\n",
        "            grad1 = np.dot(inp.T, grad) / true.shape[0]\n",
        "            layer[\"grad\"] = grad1\n",
        "        if type == \"conv\":\n",
        "            grad1 = conv(inp, grad)\n",
        "            layer[\"grad\"] = grad1\n",
        "            if layer[\"weight\"].shape != layer[\"grad\"].shape:\n",
        "                print(layer[\"weight\"].shape, layer[\"grad\"].shape, layer[\"type\"])\n",
        "    return layers\n",
        "\n",
        "def propagate(layers, lr, itr):\n",
        "    \"\"\"Perform parameter updates using the propagated gradients through the layers.\n",
        "\n",
        "    Args:\n",
        "        layers (list): List of dictionaries representing each layer in the network.\n",
        "        lr (float): Learning rate.\n",
        "        itr (int): Current iteration.\n",
        "\n",
        "    Returns:\n",
        "        list: Updated list of dictionaries with updated weights based on the gradients.\n",
        "    \"\"\"\n",
        "    for layer in layers:\n",
        "        if layer[\"type\"] != \"flatten\" and layer[\"type\"] != \"max\":\n",
        "            try:\n",
        "                layer[\"vdw\"] = 0.9 * layer[\"vdw\"] + 0.1 * layer[\"grad\"]\n",
        "                layer[\"sdw\"] = 0.99 * layer[\"sdw\"] + 0.01 * layer[\"grad\"] ** 2\n",
        "            except:\n",
        "                layer[\"vdw\"] = 0.1 * layer[\"grad\"]\n",
        "                layer[\"sdw\"] = 0.01 * layer[\"grad\"] ** 2\n",
        "            layer[\"vcorr\"] = layer[\"vdw\"] / (0.9**itr)\n",
        "            layer[\"scorr\"] = layer[\"sdw\"] / (0.99**itr)\n",
        "            layer[\"weight\"] -= lr * layer[\"vcorr\"] / ((layer[\"scorr\"] ** 0.5) + 1e-6)\n",
        "    return layers\n",
        "\n",
        "\n",
        "def update(layers):\n",
        "    \"\"\"Update the outputs of each layer in the neural network.\n",
        "\n",
        "    Args:\n",
        "        layers (list): List of dictionaries representing each layer in the network.\n",
        "\n",
        "    Returns:\n",
        "        list: Updated list of dictionaries with the updated outputs after the forward pass.\n",
        "    \"\"\"\n",
        "    layers = layers[::-1]\n",
        "    for en, layer in enumerate(layers):\n",
        "        try:\n",
        "            wts, inp, out, type, pos = (\n",
        "                layer[\"weight\"],\n",
        "                layer[\"input\"],\n",
        "                layer[\"output\"],\n",
        "                layer[\"type\"],\n",
        "                layer[\"position\"],\n",
        "            )\n",
        "        except:\n",
        "            try:\n",
        "                out, shp, type, pos = (\n",
        "                    layer[\"output\"],\n",
        "                    layer[\"shape\"],\n",
        "                    layer[\"type\"],\n",
        "                    layer[\"position\"],\n",
        "                )\n",
        "            except:\n",
        "                out, type, pos, shp, index = (\n",
        "                    layer[\"output\"],\n",
        "                    layer[\"type\"],\n",
        "                    layer[\"position\"],\n",
        "                    layer[\"kernel\"],\n",
        "                    layer[\"index\"],\n",
        "                )\n",
        "        if type == \"conv\":\n",
        "            bs, a, b, c = inp.shape\n",
        "            bs, x, y, z = out.shape\n",
        "            kernel = wts.shape[1]\n",
        "            for i in range(z):\n",
        "                for j in range(x):\n",
        "                    for k in range(y):\n",
        "                        out[:, j, k, i] = dot(\n",
        "                            wts[:, :, :, i], inp[:, j : j + kernel, k : k + kernel]\n",
        "                        )\n",
        "        if type == \"out\":\n",
        "            out = np.dot(inp, wts)\n",
        "            out = sigmoid(out)\n",
        "        if pos != 1:\n",
        "            layers[en + 1][\"input\"] = out\n",
        "        layer[\"output\"] = out\n",
        "    return layers[::-1]\n",
        "def get_data(data_dir, IMG_WIDTH, IMG_HEIGHT):\n",
        "    \"\"\"Load and preprocess image data from the given directory.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): The directory containing the image data.\n",
        "        IMG_WIDTH (int): The target width for resizing the images.\n",
        "        IMG_HEIGHT (int): The target height for resizing the images.\n",
        "\n",
        "    Returns:\n",
        "        np.array: An array containing preprocessed image data and corresponding labels.\n",
        "    \"\"\"\n",
        "    labels = [\"PNEUMONIA\", \"NORMAL\"]\n",
        "    data = []\n",
        "    for label in labels:\n",
        "        path = os.path.join(data_dir, label)\n",
        "        class_num = labels.index(label)\n",
        "        for en, img in enumerate(os.listdir(path)):\n",
        "            if en == 0:\n",
        "                print(img)  # Print the first image in each class, for debugging purposes\n",
        "            try:\n",
        "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
        "                resized_arr = cv2.resize(img_arr, (IMG_WIDTH, IMG_HEIGHT))\n",
        "                data.append([resized_arr, class_num])\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "    return np.array(data)\n",
        "\n",
        "def loss(true, pred):\n",
        "    \"\"\"Calculate the binary cross-entropy loss.\n",
        "\n",
        "    Args:\n",
        "        true (np.array): The true labels.\n",
        "        pred (np.array): The predicted probabilities.\n",
        "\n",
        "    Returns:\n",
        "        float: The mean binary cross-entropy loss.\n",
        "    \"\"\"\n",
        "    return np.mean(-1 * ((true * np.log(pred)) + ((1 - true) * (np.log(1 - pred)))))\n",
        "\n",
        "\n",
        "# Constants\n",
        "filter = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]])\n",
        "BATCH_SIZE = 32\n",
        "IMG_HEIGHT = 240\n",
        "IMG_WIDTH = 240\n",
        "ALPHA = 2e-4\n",
        "\n",
        "# Load and preprocess training data\n",
        "train = get_data(\"../input/chest-xray-pneumonia/chest_xray/chest_xray/train\")\n",
        "\n",
        "trn = []\n",
        "tar = []\n",
        "for i in tqdm(range(5216)):\n",
        "    img = train[i][0]\n",
        "\n",
        "    trn.append(img.reshape((1, 240, 240, 1)))\n",
        "    tar.append(train[i][1])\n",
        "\n",
        "trn = np.asarray(trn) / 255\n",
        "tar = np.asarray(tar)\n",
        "\n",
        "trn = trn.reshape((5216, 240, 240, 1))\n",
        "\n",
        "out = []\n",
        "true = []\n",
        "for e in tqdm(range(int(math.ceil(trn.shape[0] / 32)))):\n",
        "    img = []\n",
        "    target = []\n",
        "    for ii in range(e * 32, (e + 1) * 32):\n",
        "        if ii < trn.shape[0]:\n",
        "            img.append(trn[ii].reshape(240, 240, 1))\n",
        "            target.append(tar[ii])\n",
        "    img = np.asarray(img)\n",
        "    if e == 0 or e == 19:\n",
        "        # Some intermediate layers\n",
        "        a = convolution(img, 2, 2)\n",
        "        b = convolution(a[2], 2, 4)\n",
        "        g = maxpool(b[2], 2)\n",
        "        f = flatten(g[0])\n",
        "        o = dense(f[1], 1)\n",
        "        layers = [\n",
        "            {\n",
        "                \"weight\": np.load(\"../input/makes-sense-mean-1/out06_weight.npy\"),\n",
        "                \"input\": o[1],\n",
        "                \"output\": o[2],\n",
        "                \"type\": \"out\",\n",
        "                \"position\": 1,\n",
        "            },\n",
        "            {\"shape\": f[0].shape, \"output\": f[1], \"type\": \"flatten\", \"position\": 2},\n",
        "            {\n",
        "                \"output\": g[0],\n",
        "                \"kernel\": g[1],\n",
        "                \"index\": g[2],\n",
        "                \"type\": \"max\",\n",
        "                \"position\": 3,\n",
        "                \"input\": g[-1],\n",
        "            },\n",
        "            {\n",
        "                \"weight\": np.load(\"../input/makes-sense-mean-1/conv36_weight.npy\"),\n",
        "                \"input\": b[1],\n",
        "                \"output\": b[2],\n",
        "                \"type\": \"conv\",\n",
        "                \"position\": 4,\n",
        "            },\n",
        "            {\n",
        "                \"weight\": np.load(\"../input/makes-sense-mean-1/conv46_weight.npy\"),\n",
        "                \"input\": img,\n",
        "                \"output\": a[2],\n",
        "                \"type\": \"conv\",\n",
        "                \"position\": 5,\n",
        "            },\n",
        "        ]\n",
        "\n",
        "    layers[-1][\"input\"] = img\n",
        "    layers = update(layers)\n",
        "    if e == 0:\n",
        "        out = layers[0][\"output\"]\n",
        "    else:\n",
        "        out = np.concatenate([out, layers[0][\"output\"]])\n",
        "    true.append(target)\n",
        "\n",
        "loss(tar, out)\n",
        "# Save predictions and targets for the training set\n",
        "np.save(\"predictions_train.npy\", out)\n",
        "np.save(\"train_targets.npy\", tar)\n",
        "\n",
        "# Load and preprocess test data\n",
        "train = get_data(\"../input/chest-xray-pneumonia/chest_xray/chest_xray/test\")\n",
        "\n",
        "trn = []\n",
        "tar = []\n",
        "for i in tqdm(range(624)):\n",
        "    img = train[i][0]\n",
        "    trn.append(img.reshape((1, 240, 240, 1)))\n",
        "    tar.append(train[i][1])\n",
        "\n",
        "trn = np.asarray(trn) / 255\n",
        "tar = np.asarray(tar)\n",
        "\n",
        "out = []\n",
        "true = []\n",
        "for e in tqdm(range(int(math.ceil(trn.shape[0] / 32)))):\n",
        "    img = []\n",
        "    target = []\n",
        "    for ii in range(e * 32, (e + 1) * 32):\n",
        "        if ii < trn.shape[0]:\n",
        "            img.append(trn[ii].reshape(240, 240, 1))\n",
        "            target.append(tar[ii])\n",
        "    img = np.asarray(img)\n",
        "\n",
        "    if e == 0 or e == 19:\n",
        "        # Some intermediate layers\n",
        "        a = convolution(img, 2, 2)\n",
        "        b = convolution(a[2], 2, 4)\n",
        "        g = maxpool(b[2], 2)\n",
        "        f = flatten(g[0])\n",
        "        o = dense(f[1], 1)\n",
        "        layers = [\n",
        "            {\n",
        "                \"weight\": np.load(\"../input/makes-sense-mean-1/out06_weight.npy\"),\n",
        "                \"input\": o[1],\n",
        "                \"output\": o[2],\n",
        "                \"type\": \"out\",\n",
        "                \"position\": 1,\n",
        "            },\n",
        "            {\"shape\": f[0].shape, \"output\": f[1], \"type\": \"flatten\", \"position\": 2},\n",
        "            {\n",
        "                \"output\": g[0],\n",
        "                \"kernel\": g[1],\n",
        "                \"index\": g[2],\n",
        "                \"type\": \"max\",\n",
        "                \"position\": 3,\n",
        "                \"input\": g[-1],\n",
        "            },\n",
        "            {\n",
        "                \"weight\": np.load(\"../input/makes-sense-mean-1/conv36_weight.npy\"),\n",
        "                \"input\": b[1],\n",
        "                \"output\": b[2],\n",
        "                \"type\": \"conv\",\n",
        "                \"position\": 4,\n",
        "            },\n",
        "            {\n",
        "                \"weight\": np.load(\"../input/makes-sense-mean-1/conv46_weight.npy\"),\n",
        "                \"input\": img,\n",
        "                \"output\": a[2],\n",
        "                \"type\": \"conv\",\n",
        "                \"position\": 5,\n",
        "            },\n",
        "        ]\n",
        "\n",
        "    layers[-1][\"input\"] = img\n",
        "    layers = update(layers)\n",
        "    if e == 0:\n",
        "        out = layers[0][\"output\"]\n",
        "    else:\n",
        "        out = np.concatenate([out, layers[0][\"output\"]])\n",
        "    true.append(target)\n",
        "\n",
        "# Calculate the loss between the true targets and the predicted outputs\n",
        "loss(tar, out)\n",
        "\n",
        "\n",
        "# Save predictions and targets for the test set\n",
        "np.save(\"predictions_test.npy\", out)\n",
        "np.save(\"test_targets.npy\", tar)\n",
        "\n",
        "# Load predictions and targets for the training set\n",
        "a = np.load(\"predictions_train.npy\")\n",
        "b = np.load(\"train_targets.npy\")\n",
        "\n",
        "# Calculate accuracy score for the training set\n",
        "train_accuracy = accuracy_score(np.round(a), b)\n",
        "\n",
        "# Load predictions and targets for the test set\n",
        "a = np.load(\"predictions_test.npy\")\n",
        "b = np.load(\"test_targets.npy\")\n",
        "\n",
        "# Calculate accuracy score for the test set\n",
        "test_accuracy = accuracy_score(np.round(a), b)\n",
        "\n",
        "# Load loss values\n",
        "a = np.load(\"../input/makes-sense-mean-1/loss.npy\")\n",
        "\n",
        "# Plot the loss values\n",
        "plt.plot(a[:-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}